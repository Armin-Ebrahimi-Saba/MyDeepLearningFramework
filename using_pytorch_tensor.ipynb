{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b3135dbed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6614, 0.2669, 0.0617]]])\n",
      "tensor([[[ 0.6213, -0.4519, -0.1661]]])\n",
      "tensor([[[-1.5228,  0.3817, -1.0276]]])\n",
      "tensor([[[-0.5631, -0.8923, -0.0583]]])\n",
      "tensor([[[-0.1955, -0.9656,  0.4224]]])\n"
     ]
    }
   ],
   "source": [
    "inp = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "for i in inp:\n",
    "    print(i.view(1, 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.1764, 0.0058, 0.3312]]], grad_fn=<StackBackward0>), tensor([[[0.4567, 0.0160, 0.6298]]], grad_fn=<StackBackward0>))\n",
      "tensor([[ 0.2673, -0.4057,  1.1341],\n",
      "        [-1.1115,  0.3501, -0.7703],\n",
      "        [-0.1473,  0.6272,  1.0935],\n",
      "        [ 0.0939,  1.2381, -1.3459],\n",
      "        [ 0.5119, -0.6933, -0.1668]])\n",
      "inputs=tensor([[[ 0.2673, -0.4057,  1.1341]],\n",
      "\n",
      "        [[-1.1115,  0.3501, -0.7703]],\n",
      "\n",
      "        [[-0.1473,  0.6272,  1.0935]],\n",
      "\n",
      "        [[ 0.0939,  1.2381, -1.3459]],\n",
      "\n",
      "        [[ 0.5119, -0.6933, -0.1668]]])\n",
      "(tensor([[[ 0.1518, -0.0223,  0.3176]]], grad_fn=<StackBackward0>), tensor([[[ 0.3891, -0.0610,  0.6080]]], grad_fn=<StackBackward0>))\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "# not correct this is both hidden and cell state\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "# print(out[-1])\n",
    "print(hidden)\n",
    "# print(torch.all(torch.eq(out, hidden[0])))\n",
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "print(torch.cat(inputs))\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(f'{inputs=}')\n",
    "# print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "class Linear:\n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
    "    self.bias = torch.randn(fan_out) / fan_out**0.5 if bias else None # note: kaiming init\n",
    "    # self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    # print(f'{self.out.shape=}')\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "  # 1/m x.sum(dim)                  mean\n",
    "  # torch.sum((x - mean)**2)/(m-1)  variance\n",
    "  # (x-mean)/torch.sqrt(var+eps)    normalize\n",
    "  # gamma*x + beta                  scale and shift\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      if x.ndim == 2:\n",
    "        dim = 0\n",
    "      elif x.ndim == 3:\n",
    "        dim = (0,1)\n",
    "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
    "      xvar = x.var(dim, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX]\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      if type(layer) is LSTM:\n",
    "        x, _ = layer(x)\n",
    "      else:\n",
    "        x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # get parameters of all layers and stretch them out into one list\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "class Sigmoid:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.sigmoid(x)\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "  \n",
    "# ------------------------------------------------------------------------------------------------\n",
    "class LSTM:\n",
    "  def __init__(self, input_size, hidden_size, bias=True):\n",
    "    self.f_W = torch.randn((input_size, hidden_size)) / input_size**0.5   # note: kaiming init\n",
    "    self.f_U = torch.randn((hidden_size, hidden_size)) / hidden_size**0.5 # note: kaiming init\n",
    "    # self.f_bias = torch.zeros(hidden_size) if bias else None\n",
    "    self.f_bias = torch.randn(hidden_size) / hidden_size**0.5 if bias else None # note: kaiming init\n",
    "\n",
    "    self.m_W = torch.randn((input_size, hidden_size)) / input_size**0.5   # note: kaiming init\n",
    "    self.m_U = torch.randn((hidden_size, hidden_size)) / hidden_size**0.5 # note: kaiming init\n",
    "    # self.m_bias = torch.zeros(hidden_size) if bias else None\n",
    "    self.m_bias = torch.randn(hidden_size) / hidden_size**0.5 if bias else None # note: kaiming init\n",
    "\n",
    "    self.i_W = torch.randn((input_size, hidden_size)) / input_size**0.5   # note: kaiming init\n",
    "    self.i_U = torch.randn((hidden_size, hidden_size)) / hidden_size**0.5 # note: kaiming init\n",
    "    # self.i_bias = torch.zeros(hidden_size) if bias else None\n",
    "    self.i_bias = torch.randn(hidden_size) / hidden_size**0.5 if bias else None # note: kaiming init\n",
    "\n",
    "    self.o_W = torch.randn((input_size, hidden_size)) / input_size**0.5   # note: kaiming init\n",
    "    self.o_U = torch.randn((hidden_size, hidden_size)) / hidden_size**0.5 # note: kaiming init\n",
    "    # self.o_bias = torch.zeros(hidden_size) if bias else None\n",
    "    self.o_bias = torch.randn(hidden_size) / hidden_size**0.5 if bias else None # note: kaiming init\n",
    "\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    self.bias = bias\n",
    "    self.training = True\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    B, T, _ = x.shape                               # T is the sequence size \n",
    "    h_n = torch.tensor([])\n",
    "    h = torch.zeros(B, self.hidden_size)\n",
    "    c = torch.zeros(B, self.hidden_size)\n",
    "    for t in range(T):\n",
    "      x_t = x[:, t, :]\n",
    "      forget = x_t @ self.f_W + h @ self.f_U   # forget gate\n",
    "      add = x_t @ self.i_W + h @ self.i_U      # input/add gate\n",
    "      mask = x_t @ self.m_W + h @ self.m_U     # input gate\n",
    "      output = x_t @ self.o_W + h @ self.o_U   # output gate\n",
    "      if self.bias:\n",
    "        forget += self.f_bias\n",
    "        add += self.i_bias\n",
    "        mask += self.m_bias\n",
    "        output += self.o_bias\n",
    "      forget = torch.sigmoid(forget)\n",
    "      add = torch.tanh(add)\n",
    "      mask = torch.sigmoid(forget)\n",
    "      output = torch.sigmoid(output)\n",
    "      c = c * forget + add * mask\n",
    "      h = torch.tanh(c) * output\n",
    "      h_n = torch.cat((h_n, h.unsqueeze(1)), dim=1)\n",
    "    return h_n, (h, c)\n",
    "\n",
    "  def train(self):\n",
    "    self.training = True\n",
    "  \n",
    "  def eval(self):\n",
    "    self.training = False\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.f_W, self.m_W, self.i_W, self.o_W, self.f_U, self.m_U, self.i_U, self.o_U] +\\\n",
    "  ([self.f_bias, self.m_bias, self.i_bias, self.o_bias] if self.bias else [] )\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "class Head(nn.Module):\n",
    "  def __init__(self, n_embed, head_size, block_size, dropout_rate=0.0) -> None:\n",
    "    super().__init__()\n",
    "    self.query = nn.Linear(n_embed, head_size, bias=True)\n",
    "    self.key = nn.Linear(n_embed, head_size, bias=True)\n",
    "    self.value = nn.Linear(n_embed, head_size, bias=True)\n",
    "    self.register_buffer('tril', torch.tril(torch.ones(block_size, head_size))) ####################################\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    B, T, C = x.shape\n",
    "    k = self.key(x)     # B, T, head_size\n",
    "    q = self.query(x)   # B, T, head_size\n",
    "    wei = q @ k.transpose(-1, -2) * k.size(-1)**-0.5  # B, T, T\n",
    "    wei = wei.masked_fill(self.tril==0, float('-inf'))\n",
    "    wei = F.softmax(wei, dim=-1) * (1.0/torch.sqrt(k.size(-1)))   # B, T, T\n",
    "    wei = self.dropout(wei)\n",
    "    v = self.value(x)   # B, T, head_size\n",
    "    out = wei @ v    # B, T, head_size\n",
    "    return out\n",
    "  \n",
    "# ------------------------------------------------------------------------------------------------\n",
    "class MultiheadAttention(nn.Module):\n",
    "  def __init__(self, n_embed, head_size, block_size, n_head, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.heads = nn.ModuleList([Head(n_embed, head_size, block_size, dropout_rate) for _ in range(n_head)])\n",
    "    self.proj = nn.Linear(head_size * n_head, n_embed)\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = torch.concat([head(x) for head in self.heads], dim=-1)\n",
    "    return self.dropout(self.proj(out))\n",
    "  \n",
    "# ------------------------------------------------------------------------------------------------\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, n_embed, dropout_rate=0.0, bias=True):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Linear(n_embed, 4 * n_embed, bias),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4 * n_embed, n_embed, bias),\n",
    "        nn.Dropout(dropout_rate),\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.net(x)\n",
    "    \n",
    "# ------------------------------------------------------------------------------------------------\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, n_embed, head_size, block_size, n_head, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.sa = MultiheadAttention(n_embed, head_size, block_size, n_head, dropout_rate)\n",
    "    self.ffw = FeedForward(n_embed, dropout_rate)\n",
    "    self.ln1 = nn.LayerNorm(n_embed)\n",
    "    self.ln2 = nn.LayerNorm(n_embed)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = self.ln1(x + self.sa(x))\n",
    "    out = self.ln2(out + self.ffw(out))\n",
    "    return out\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, n_embed, head_size, block_size, n_head, n_block_size, vocab_size, num_layers, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "    self.positional_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "    self.decoders = nn.Sequential(*[Decoder(n_embed, head_size, block_size, n_head, n_block_size, dropout_rate) for _ in range(num_layers)])\n",
    "    self.ln = nn.LayerNorm(n_embed)\n",
    "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "  \n",
    "  def forward(self, x):    \n",
    "    tok_emb = self.token_embedding_table(x)\n",
    "    pos_emb = self.positional_embedding_table(x)\n",
    "    emb = tok_emb + pos_emb\n",
    "    probs = F.softmax(self.lm_head(self.decoders(emb)), dim=-1)\n",
    "    return probs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jja = torch.randn((4, 3, 2))\n",
    "(a.transpose(-2, -1) == a.transpose(-1, -2)).all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\armin\\AppData\\Local\\Temp\\ipykernel_64720\\1375659546.py:22: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  return torch.tensor(X), torch.tensor(y)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/airline-passengers.csv')\n",
    "timeseries = df[[\"Passengers\"]].values.astype('float32')\n",
    "\n",
    "# train-test split for time series\n",
    "train_size = int(len(timeseries) * 0.67)\n",
    "test_size = len(timeseries) - train_size\n",
    "train, test = timeseries[:train_size], timeseries[train_size:]\n",
    "\n",
    "def create_dataset(dataset, lookback):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset)-lookback):\n",
    "        feature = dataset[i:i+lookback]\n",
    "        target = dataset[i+1:i+lookback+1]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "lookback = 4\n",
    "X_train, y_train = create_dataset(train, lookback=lookback)\n",
    "X_test, y_test = create_dataset(test, lookback=lookback)\n",
    "\n",
    "class AirModel():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = LSTM(input_size=1, hidden_size=50)\n",
    "        self.linear = Linear(50, 1)\n",
    "        self.layers = Sequential([self.lstm, self.linear])\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.layers.parameters()\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "model = AirModel()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=8) #, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train RMSE 226.5480, test RMSE 425.8252\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# for param in model.parameters():\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#     param.grad = None\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\armin\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\armin\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "for epoch in range(n_epochs):\n",
    "    for X_batch, y_batch in loader:\n",
    "        # print(f\"{X_batch.shape=}\")\n",
    "        y_pred = model(X_batch)\n",
    "        # print(f'{y_pred.shape=}')\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        # for param in model.parameters():\n",
    "        #     param.grad = None\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    if epoch % 100 != 0:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_train)\n",
    "        train_rmse = np.sqrt(loss_fn(y_pred, y_train))\n",
    "        y_pred = model(X_test)\n",
    "        test_rmse = np.sqrt(loss_fn(y_pred, y_test))\n",
    "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))\n",
    "\n",
    "with torch.no_grad():\n",
    "    # shift train predictions for plotting\n",
    "    train_plot = np.ones_like(timeseries) * np.nan\n",
    "    y_pred = model(X_train)\n",
    "    y_pred = y_pred[:, -1, :]\n",
    "    train_plot[lookback:train_size] = model(X_train)[:, -1, :]\n",
    "    # shift test predictions for plotting\n",
    "    test_plot = np.ones_like(timeseries) * np.nan\n",
    "    test_plot[train_size+lookback:len(timeseries)] = model(X_test)[:, -1, :]\n",
    "# plot\n",
    "plt.plot(timeseries)\n",
    "plt.plot(train_plot, c='r')\n",
    "plt.plot(test_plot, c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [      nan]\n",
      " [312.84384]\n",
      " [313.51767]\n",
      " [314.45172]\n",
      " [314.4714 ]\n",
      " [314.4823 ]\n",
      " [314.45532]\n",
      " [312.75214]\n",
      " [291.3717 ]\n",
      " [312.98538]\n",
      " [311.8894 ]\n",
      " [302.96506]\n",
      " [314.01797]\n",
      " [312.86243]\n",
      " [313.92993]\n",
      " [314.46118]\n",
      " [314.4745 ]\n",
      " [314.48462]\n",
      " [314.45596]\n",
      " [313.75656]\n",
      " [295.5896 ]\n",
      " [312.78143]\n",
      " [313.81253]\n",
      " [312.01944]\n",
      " [314.42624]\n",
      " [314.41226]\n",
      " [314.46613]\n",
      " [314.4842 ]\n",
      " [314.4866 ]\n",
      " [314.4889 ]\n",
      " [314.489  ]\n",
      " [314.4625 ]\n",
      " [313.89755]\n",
      " [314.44974]\n",
      " [314.46457]\n",
      " [314.4033 ]\n",
      " [314.47205]\n",
      " [314.48355]\n",
      " [314.48566]\n",
      " [314.48856]\n",
      " [314.4895 ]\n",
      " [314.48996]\n",
      " [314.49008]\n",
      " [314.4891 ]\n",
      " [314.4054 ]]\n"
     ]
    }
   ],
   "source": [
    "print(test_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "model.parameters()[i]=tensor([[-0.9640,  0.1619, -0.1637, -0.3582, -0.0594, -2.4919,  0.2423,  0.2883,\n",
      "          0.0558,  1.1004, -0.3417,  0.9473,  0.6223, -0.4481,  0.0581,  0.3880,\n",
      "          0.5149, -1.8475, -2.9167, -0.5673, -1.1992,  0.0624, -2.0030, -0.4914,\n",
      "         -1.5458, -0.1733,  0.7282,  0.0571, -0.7092, -0.5262, -1.1042, -0.6994,\n",
      "          0.2352,  1.9142,  0.3909,  0.3872,  2.6415, -0.9624, -0.2076, -1.3889,\n",
      "          0.0657, -1.8734, -0.9295,  0.2936,  1.6604,  0.2717, -0.8087,  0.1590,\n",
      "          0.5707, -0.1348]], requires_grad=True)\n",
      "1\n",
      "model.parameters()[i]=tensor([[-0.4341,  1.1730, -0.0338,  0.8725, -0.4399,  0.7231,  1.3406,  0.4398,\n",
      "         -1.1559,  0.0697, -0.6234,  0.0500,  0.0440, -0.7867,  0.4021,  0.3452,\n",
      "          0.6277,  0.8379,  0.0947, -2.3459,  0.0666, -0.5394, -0.1718,  0.8064,\n",
      "          0.4527,  0.1162,  0.7146, -0.2477, -0.5456, -1.5167, -0.5246, -0.4189,\n",
      "          1.1212, -2.4818,  0.7760,  0.7543,  1.9706, -0.2298,  0.2130,  0.0496,\n",
      "          0.7141,  1.0593,  1.2426,  1.3821, -0.3211,  0.0546,  0.8058,  1.2334,\n",
      "         -2.7309, -2.6504]], requires_grad=True)\n",
      "2\n",
      "model.parameters()[i]=tensor([[-0.4856, -2.2527, -0.7943, -0.0410,  1.0232, -0.0255,  0.1006, -0.0843,\n",
      "         -1.1683,  0.5223, -1.1951,  1.3275, -0.0135,  0.0413, -0.8004, -0.8911,\n",
      "          2.1974, -0.0154,  1.3728, -1.9374, -0.2343,  1.2735,  1.1793,  1.3332,\n",
      "          0.3024, -0.1569, -0.4555,  0.8905, -0.7113, -1.1811,  1.2046,  0.2962,\n",
      "         -1.4638, -1.1276, -0.5869, -0.7598,  0.6226, -1.2243,  1.8728, -0.3923,\n",
      "         -1.3937,  0.5932, -0.5157,  1.8822,  0.5504, -0.9410,  0.5771,  0.6621,\n",
      "         -0.7764, -1.5427]], requires_grad=True)\n",
      "3\n",
      "model.parameters()[i]=tensor([[ 1.5140,  1.5413, -0.9555,  1.5663, -1.0875, -2.2137, -0.0722,  0.0583,\n",
      "          0.3228,  0.8286,  0.0575, -0.9356,  0.7406,  0.9143,  0.9816, -0.8463,\n",
      "          0.7358, -0.7021, -0.9613, -0.3885,  0.4454,  1.2604, -0.6674,  0.1563,\n",
      "         -0.5264, -1.7420,  1.2779, -0.6003,  0.1019, -0.3246, -1.3978, -0.6513,\n",
      "         -0.4476,  0.9136, -0.2588, -0.4840,  0.5867,  1.2809, -1.9320,  0.1589,\n",
      "          1.0768,  0.5689, -0.4487,  0.1123,  0.7337,  0.9820, -1.5713,  1.8094,\n",
      "          0.6564, -0.8383]], requires_grad=True)\n",
      "4\n",
      "model.parameters()[i]=tensor([[-0.0325, -0.0031,  0.0204,  ..., -0.1198, -0.1238, -0.0669],\n",
      "        [-0.1065, -0.0655,  0.0947,  ..., -0.3460, -0.0653, -0.0521],\n",
      "        [ 0.0521,  0.0686,  0.0415,  ...,  0.0657, -0.4779,  0.1376],\n",
      "        ...,\n",
      "        [-0.0732, -0.1110, -0.1989,  ...,  0.0803, -0.0692,  0.0864],\n",
      "        [-0.2819, -0.1169,  0.1019,  ..., -0.0109,  0.0307,  0.2275],\n",
      "        [-0.1284,  0.2541, -0.0829,  ...,  0.0978, -0.1098,  0.0051]],\n",
      "       requires_grad=True)\n",
      "5\n",
      "model.parameters()[i]=tensor([[-0.3006,  0.1255, -0.0570,  ...,  0.0967, -0.2320,  0.2328],\n",
      "        [ 0.0865, -0.0708,  0.0358,  ...,  0.0202, -0.1329, -0.3507],\n",
      "        [ 0.0475,  0.0291, -0.3837,  ..., -0.4260,  0.0095,  0.2209],\n",
      "        ...,\n",
      "        [-0.0464,  0.1826,  0.2547,  ..., -0.0528, -0.2421,  0.0065],\n",
      "        [ 0.1495, -0.0949, -0.0079,  ...,  0.2664, -0.0130, -0.0803],\n",
      "        [ 0.3716, -0.0489, -0.2418,  ..., -0.2545,  0.1840,  0.1594]],\n",
      "       requires_grad=True)\n",
      "6\n",
      "model.parameters()[i]=tensor([[-0.2321,  0.0190, -0.0601,  ..., -0.0559, -0.0674, -0.2502],\n",
      "        [ 0.0923,  0.0505,  0.0394,  ...,  0.1894, -0.0098,  0.2379],\n",
      "        [-0.2479, -0.0010,  0.0688,  ..., -0.0448, -0.0042,  0.0665],\n",
      "        ...,\n",
      "        [-0.0258,  0.1646,  0.0668,  ..., -0.0545, -0.0057, -0.2629],\n",
      "        [ 0.0214, -0.0160, -0.2442,  ...,  0.0753,  0.0363, -0.1301],\n",
      "        [ 0.0993, -0.0315, -0.1624,  ..., -0.1368,  0.0669,  0.0272]],\n",
      "       requires_grad=True)\n",
      "7\n",
      "model.parameters()[i]=tensor([[-0.0285, -0.0631,  0.0339,  ...,  0.0708, -0.1598, -0.0664],\n",
      "        [-0.1797, -0.0855,  0.1116,  ...,  0.1415, -0.0956,  0.0568],\n",
      "        [ 0.0147,  0.0159,  0.0558,  ...,  0.0005,  0.0965, -0.1834],\n",
      "        ...,\n",
      "        [ 0.0178,  0.2340, -0.1846,  ..., -0.0463,  0.1952, -0.0360],\n",
      "        [-0.2632, -0.0477, -0.2827,  ...,  0.0361,  0.0548, -0.0180],\n",
      "        [ 0.2857, -0.0299, -0.2257,  ..., -0.2608, -0.1728, -0.4311]],\n",
      "       requires_grad=True)\n",
      "8\n",
      "model.parameters()[i]=tensor([-1.1389e-01,  5.2240e-02,  2.3802e-01,  3.0586e-02,  9.9507e-02,\n",
      "         2.5349e-01, -2.0727e-01, -2.5118e-01, -1.9032e+00,  1.3308e-02,\n",
      "         3.2520e-01, -4.6671e-02,  8.2244e-03, -8.1829e-02, -1.5286e+00,\n",
      "        -1.6246e-01,  1.2601e-01,  9.8921e-02,  5.6972e-02, -9.3141e-02,\n",
      "        -1.2010e-01, -1.5593e+00,  2.6713e-01,  1.6065e-01,  1.0084e-01,\n",
      "         9.4604e-02, -1.2533e-01, -1.8224e-01, -8.2642e-02,  1.8745e-01,\n",
      "         1.7079e-01, -7.5437e-02, -3.1052e-02,  1.0115e-01,  5.0840e-02,\n",
      "        -5.3441e-02,  3.2620e-02,  3.4087e-02,  2.4270e-01,  1.4209e-01,\n",
      "        -1.4771e+00, -2.3437e-01, -5.1431e-02,  9.2424e-02, -1.8204e-01,\n",
      "        -8.3344e-02, -4.8081e-02,  1.8609e-01, -7.5333e-02, -3.5630e-04],\n",
      "       requires_grad=True)\n",
      "9\n",
      "model.parameters()[i]=tensor([-0.1251,  0.0808, -0.0737, -0.2671, -0.1180, -0.2394, -0.2182, -0.0350,\n",
      "         0.1783, -0.0810,  0.2692, -0.0720,  0.2147,  0.0300,  0.1309,  0.0443,\n",
      "        -0.1137,  0.0947, -0.1137, -0.1295, -0.0687, -0.1012, -0.1709, -0.1314,\n",
      "        -0.1504, -0.3450, -0.1248,  0.0293, -0.3018, -0.0496,  0.1026, -0.1058,\n",
      "         0.0053,  0.0865, -0.1498, -0.0105,  0.1275,  0.0217, -0.0593, -0.0195,\n",
      "        -0.1206, -0.1263, -0.2595, -0.2328,  0.3021, -0.1102,  0.2018, -0.0518,\n",
      "         0.2219, -0.0176], requires_grad=True)\n",
      "10\n",
      "model.parameters()[i]=tensor([ 0.1663,  0.1710,  0.0116,  1.6207, -0.0264,  0.1725, -0.1486, -0.2545,\n",
      "         0.2259,  0.2830,  0.0467,  0.1126,  2.7661, -1.2696,  0.0236,  0.0649,\n",
      "        -0.1030,  0.1050, -0.1604, -0.0705,  0.0708,  0.0154,  0.2505, -0.1215,\n",
      "         0.1610,  0.2297,  0.0412,  0.0522,  0.0913, -0.0526,  0.0230,  0.1707,\n",
      "        -0.0030, -0.2473, -0.0810, -0.0084,  0.0049,  0.1174, -0.2003, -0.0028,\n",
      "        -0.1448,  0.1127,  0.0033, -0.0777, -0.0217, -0.1415,  0.0726, -0.2433,\n",
      "        -0.2682,  0.0357], requires_grad=True)\n",
      "11\n",
      "model.parameters()[i]=tensor([ 0.0383,  0.3409, -0.0419,  0.0266,  0.0133, -0.0342,  1.2780, -1.7229,\n",
      "         0.0818, -0.0790, -1.7907, -0.1607,  0.0507,  0.1097, -0.1732,  0.0350,\n",
      "        -0.2164,  0.0616, -0.2390,  0.2097,  0.0508, -0.1851,  0.1279,  0.4037,\n",
      "         0.0786,  0.1614,  0.1067,  0.2051, -1.5509, -0.1515, -0.1109, -0.2306,\n",
      "         0.2560, -0.3116, -0.2639,  0.0610,  0.0777,  0.2943, -0.1113,  0.1713,\n",
      "         0.2704,  0.3020,  0.0526, -1.8127,  0.0491,  0.0370,  0.0212,  0.1122,\n",
      "        -0.1825,  0.1329], requires_grad=True)\n",
      "12\n",
      "model.parameters()[i]=tensor([[-1.2660e+01],\n",
      "        [-1.2728e+01],\n",
      "        [-1.9274e-02],\n",
      "        [-1.7690e+01],\n",
      "        [-9.8631e-02],\n",
      "        [ 1.2859e-01],\n",
      "        [-1.1787e+01],\n",
      "        [-1.6446e+01],\n",
      "        [-1.4392e+01],\n",
      "        [ 1.2893e+01],\n",
      "        [-1.6074e+01],\n",
      "        [ 2.6787e-01],\n",
      "        [-1.8513e+01],\n",
      "        [ 1.8024e+01],\n",
      "        [-1.5132e+01],\n",
      "        [-9.5905e-02],\n",
      "        [ 1.2661e+01],\n",
      "        [ 1.6164e-01],\n",
      "        [ 1.6157e-01],\n",
      "        [ 6.7963e-02],\n",
      "        [-1.2950e+01],\n",
      "        [ 1.4515e+01],\n",
      "        [-3.2364e-01],\n",
      "        [ 1.2741e+01],\n",
      "        [-9.9675e-02],\n",
      "        [-2.5204e-02],\n",
      "        [-1.2923e+01],\n",
      "        [ 3.1325e-02],\n",
      "        [-1.5595e+01],\n",
      "        [ 9.1313e-02],\n",
      "        [ 1.5694e-01],\n",
      "        [-9.5788e-03],\n",
      "        [-3.1500e-02],\n",
      "        [-1.2527e+01],\n",
      "        [ 1.3416e-01],\n",
      "        [ 4.3919e-01],\n",
      "        [ 1.2668e+01],\n",
      "        [-1.2778e+01],\n",
      "        [-2.5775e-01],\n",
      "        [-1.2722e+01],\n",
      "        [-1.4495e+01],\n",
      "        [ 1.2635e+01],\n",
      "        [ 2.0078e-01],\n",
      "        [ 1.6190e+01],\n",
      "        [ 1.2595e+01],\n",
      "        [-1.2756e+01],\n",
      "        [-1.3210e-01],\n",
      "        [ 1.2804e+01],\n",
      "        [-1.2623e+01],\n",
      "        [ 1.0769e-01]], requires_grad=True)\n",
      "13\n",
      "model.parameters()[i]=tensor([12.5072], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model.parameters())):\n",
    "    print(i)\n",
    "    print(f'{model.parameters()[i]=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1251,  0.0808, -0.0737, -0.2671, -0.1180, -0.2394, -0.2182, -0.0350,\n",
      "         0.1783, -0.0810,  0.2692, -0.0720,  0.2147,  0.0300,  0.1309,  0.0443,\n",
      "        -0.1137,  0.0947, -0.1137, -0.1295, -0.0687, -0.1012, -0.1709, -0.1314,\n",
      "        -0.1504, -0.3450, -0.1248,  0.0293, -0.3018, -0.0496,  0.1026, -0.1058,\n",
      "         0.0053,  0.0865, -0.1498, -0.0105,  0.1275,  0.0217, -0.0593, -0.0195,\n",
      "        -0.1206, -0.1263, -0.2595, -0.2328,  0.3021, -0.1102,  0.2018, -0.0518,\n",
      "         0.2219, -0.0176], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model.parameters()[9])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
