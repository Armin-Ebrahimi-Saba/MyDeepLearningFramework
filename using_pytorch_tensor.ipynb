{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "for i in inp:\n",
    "    print(i.view(1, 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "# not correct this is both hidden and cell state\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "# print(out[-1])\n",
    "print(hidden)\n",
    "# print(torch.all(torch.eq(out, hidden[0])))\n",
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "print(torch.cat(inputs))\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(f'{inputs=}')\n",
    "# print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "class Linear:\n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
    "    self.bias = torch.randn(fan_out) / fan_out**0.5 if bias else None # note: kaiming init\n",
    "    # self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    # print(f'{self.out.shape=}')\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "  # 1/m x.sum(dim)                  mean\n",
    "  # torch.sum((x - mean)**2)/(m-1)  variance\n",
    "  # (x-mean)/torch.sqrt(var+eps)    normalize\n",
    "  # gamma*x + beta                  scale and shift\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      if x.ndim == 2:\n",
    "        dim = 0\n",
    "      elif x.ndim == 3:\n",
    "        dim = (0,1)\n",
    "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
    "      xvar = x.var(dim, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX]\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      if type(layer) is LSTM:\n",
    "        x, _ = layer(x)\n",
    "      else:\n",
    "        x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    # get parameters of all layers and stretch them out into one list\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "class Sigmoid:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.sigmoid(x)\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "  \n",
    "# ------------------------------------------------------------------------------------------------\n",
    "class LSTM:\n",
    "  def __init__(self, input_size, hidden_size, bias=True):\n",
    "    self.f_W = torch.randn((input_size, hidden_size)) / input_size**0.5   # note: kaiming init\n",
    "    self.f_U = torch.randn((hidden_size, hidden_size)) / hidden_size**0.5 # note: kaiming init\n",
    "    # self.f_bias = torch.zeros(hidden_size) if bias else None\n",
    "    self.f_bias = torch.randn(hidden_size) / hidden_size**0.5 if bias else None # note: kaiming init\n",
    "\n",
    "    self.m_W = torch.randn((input_size, hidden_size)) / input_size**0.5   # note: kaiming init\n",
    "    self.m_U = torch.randn((hidden_size, hidden_size)) / hidden_size**0.5 # note: kaiming init\n",
    "    # self.m_bias = torch.zeros(hidden_size) if bias else None\n",
    "    self.m_bias = torch.randn(hidden_size) / hidden_size**0.5 if bias else None # note: kaiming init\n",
    "\n",
    "    self.i_W = torch.randn((input_size, hidden_size)) / input_size**0.5   # note: kaiming init\n",
    "    self.i_U = torch.randn((hidden_size, hidden_size)) / hidden_size**0.5 # note: kaiming init\n",
    "    # self.i_bias = torch.zeros(hidden_size) if bias else None\n",
    "    self.i_bias = torch.randn(hidden_size) / hidden_size**0.5 if bias else None # note: kaiming init\n",
    "\n",
    "    self.o_W = torch.randn((input_size, hidden_size)) / input_size**0.5   # note: kaiming init\n",
    "    self.o_U = torch.randn((hidden_size, hidden_size)) / hidden_size**0.5 # note: kaiming init\n",
    "    # self.o_bias = torch.zeros(hidden_size) if bias else None\n",
    "    self.o_bias = torch.randn(hidden_size) / hidden_size**0.5 if bias else None # note: kaiming init\n",
    "\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    self.bias = bias\n",
    "    self.training = True\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    B, T, _ = x.shape                               # T is the sequence size \n",
    "    h_n = torch.tensor([])\n",
    "    h = torch.zeros(B, self.hidden_size)\n",
    "    c = torch.zeros(B, self.hidden_size)\n",
    "    for t in range(T):\n",
    "      x_t = x[:, t, :]\n",
    "      forget = x_t @ self.f_W + h @ self.f_U   # forget gate\n",
    "      add = x_t @ self.i_W + h @ self.i_U      # input/add gate\n",
    "      mask = x_t @ self.m_W + h @ self.m_U     # input gate\n",
    "      output = x_t @ self.o_W + h @ self.o_U   # output gate\n",
    "      if self.bias:\n",
    "        forget += self.f_bias\n",
    "        add += self.i_bias\n",
    "        mask += self.m_bias\n",
    "        output += self.o_bias\n",
    "      forget = torch.sigmoid(forget)\n",
    "      add = torch.tanh(add)\n",
    "      mask = torch.sigmoid(forget)\n",
    "      output = torch.sigmoid(output)\n",
    "      c = c * forget + add * mask\n",
    "      h = torch.tanh(c) * output\n",
    "      h_n = torch.cat((h_n, h.unsqueeze(1)), dim=1)\n",
    "    return h_n, (h, c)\n",
    "\n",
    "  def train(self):\n",
    "    self.training = True\n",
    "  \n",
    "  def eval(self):\n",
    "    self.training = False\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.f_W, self.m_W, self.i_W, self.o_W, self.f_U, self.m_U, self.i_U, self.o_U] +\\\n",
    "  ([self.f_bias, self.m_bias, self.i_bias, self.o_bias] if self.bias else [] )\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "class Head(nn.Module):\n",
    "  def __init__(self, n_embed, head_size, block_size, dropout_rate=0.0) -> None:\n",
    "    super().__init__()\n",
    "    self.query = nn.Linear(n_embed, head_size, bias=True)\n",
    "    self.key = nn.Linear(n_embed, head_size, bias=True)\n",
    "    self.value = nn.Linear(n_embed, head_size, bias=True)\n",
    "    self.register_buffer('tril', torch.tril(torch.ones(block_size, head_size))) ####################################\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    B, T, C = x.shape\n",
    "    k = self.key(x)     # B, T, head_size\n",
    "    q = self.query(x)   # B, T, head_size\n",
    "    wei = q @ k.transpose(-1, -2) * k.size(-1)**-0.5  # B, T, T\n",
    "    wei = wei.masked_fill(self.tril==0, float('-inf'))\n",
    "    wei = F.softmax(wei, dim=-1) * (1.0/torch.sqrt(k.size(-1)))   # B, T, T\n",
    "    wei = self.dropout(wei)\n",
    "    v = self.value(x)   # B, T, head_size\n",
    "    out = wei @ v    # B, T, head_size\n",
    "    return out\n",
    "  \n",
    "# ------------------------------------------------------------------------------------------------\n",
    "class MultiheadAttention(nn.Module):\n",
    "  def __init__(self, n_embed, head_size, block_size, n_head, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.heads = nn.ModuleList([Head(n_embed, head_size, block_size, dropout_rate) for _ in range(n_head)])\n",
    "    self.proj = nn.Linear(head_size * n_head, n_embed)\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = torch.concat([head(x) for head in self.heads], dim=-1)\n",
    "    return self.dropout(self.proj(out))\n",
    "  \n",
    "# ------------------------------------------------------------------------------------------------\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, n_embed, dropout_rate=0.0, bias=True):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Linear(n_embed, 4 * n_embed, bias),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4 * n_embed, n_embed, bias),\n",
    "        nn.Dropout(dropout_rate),\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.net(x)\n",
    "    \n",
    "# ------------------------------------------------------------------------------------------------\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, n_embed, head_size, block_size, n_head, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.sa = MultiheadAttention(n_embed, head_size, block_size, n_head, dropout_rate)\n",
    "    self.ffw = FeedForward(n_embed, dropout_rate)\n",
    "    self.ln1 = nn.LayerNorm(n_embed)\n",
    "    self.ln2 = nn.LayerNorm(n_embed)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = self.ln1(x + self.sa(x))\n",
    "    out = self.ln2(out + self.ffw(out))\n",
    "    return out\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, n_embed, head_size, block_size, n_head, n_block_size, vocab_size, num_layers, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "    self.positional_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "    self.decoders = nn.Sequential(*[Decoder(n_embed, head_size, block_size, n_head, n_block_size, dropout_rate) for _ in range(num_layers)])\n",
    "    self.ln = nn.LayerNorm(n_embed)\n",
    "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "  \n",
    "  def forward(self, x):    \n",
    "    tok_emb = self.token_embedding_table(x)\n",
    "    pos_emb = self.positional_embedding_table(x)\n",
    "    emb = tok_emb + pos_emb\n",
    "    probs = F.softmax(self.lm_head(self.decoders(emb)), dim=-1)\n",
    "    return probs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jja = torch.randn((4, 3, 2))\n",
    "(a.transpose(-2, -1) == a.transpose(-1, -2)).all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/airline-passengers.csv')\n",
    "timeseries = df[[\"Passengers\"]].values.astype('float32')\n",
    "\n",
    "# train-test split for time series\n",
    "train_size = int(len(timeseries) * 0.67)\n",
    "test_size = len(timeseries) - train_size\n",
    "train, test = timeseries[:train_size], timeseries[train_size:]\n",
    "\n",
    "def create_dataset(dataset, lookback):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset)-lookback):\n",
    "        feature = dataset[i:i+lookback]\n",
    "        target = dataset[i+1:i+lookback+1]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "lookback = 4\n",
    "X_train, y_train = create_dataset(train, lookback=lookback)\n",
    "X_test, y_test = create_dataset(test, lookback=lookback)\n",
    "\n",
    "class AirModel():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = LSTM(input_size=1, hidden_size=50)\n",
    "        self.linear = Linear(50, 1)\n",
    "        self.layers = Sequential([self.lstm, self.linear])\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.layers.parameters()\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "model = AirModel()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=8) #, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "for epoch in range(n_epochs):\n",
    "    for X_batch, y_batch in loader:\n",
    "        # print(f\"{X_batch.shape=}\")\n",
    "        y_pred = model(X_batch)\n",
    "        # print(f'{y_pred.shape=}')\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        # for param in model.parameters():\n",
    "        #     param.grad = None\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    if epoch % 100 != 0:\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_train)\n",
    "        train_rmse = np.sqrt(loss_fn(y_pred, y_train))\n",
    "        y_pred = model(X_test)\n",
    "        test_rmse = np.sqrt(loss_fn(y_pred, y_test))\n",
    "    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))\n",
    "\n",
    "with torch.no_grad():\n",
    "    # shift train predictions for plotting\n",
    "    train_plot = np.ones_like(timeseries) * np.nan\n",
    "    y_pred = model(X_train)\n",
    "    y_pred = y_pred[:, -1, :]\n",
    "    train_plot[lookback:train_size] = model(X_train)[:, -1, :]\n",
    "    # shift test predictions for plotting\n",
    "    test_plot = np.ones_like(timeseries) * np.nan\n",
    "    test_plot[train_size+lookback:len(timeseries)] = model(X_test)[:, -1, :]\n",
    "# plot\n",
    "plt.plot(timeseries)\n",
    "plt.plot(train_plot, c='r')\n",
    "plt.plot(test_plot, c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model.parameters())):\n",
    "    print(i)\n",
    "    print(f'{model.parameters()[i]=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(model.parameters()[9])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
